{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analisi di ./LSEblog.txt\n"
     ]
    }
   ],
   "source": [
    "def leggi_contenuto(file): #la funzione restituisce in output una variabile che ha memorizzato i contenuti del file con il metodo .read()\n",
    "    with open(file, \"r\") as infile:\n",
    "        contenuto = infile.read() #il metodo .read() consente di memorizzare il contenuto del file in una variabile\n",
    "    return contenuto\n",
    "\n",
    "file1 = \"./LSEblog.txt\"\n",
    "corpus1 = leggi_contenuto(file1)\n",
    "\n",
    "print(f\"Analisi di {file1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def splitter_e_tokenizzatore(corpus): #la funzione effettua il sentence splitting e la tokenizzazione del corpus\n",
    "    frasi = nltk.tokenize.sent_tokenize(corpus) #splitta il testo in frasi\n",
    "    tokens = []\n",
    "    for frase in frasi:\n",
    "        tokens_frase = nltk.tokenize.word_tokenize(frase) #tokenizza ciascuna frase\n",
    "        tokens.extend(tokens_frase) # .extend() aggiunge in coda alla lista di tokens del corpus i tokens della frase\n",
    "    return frasi, tokens\n",
    "\n",
    "def PoS_tagger(tokens): #la funzione effettua il PoS tagging data la lista di tokens del corpus\n",
    "    tokens_PoS = nltk.tag.pos_tag(tokens)\n",
    "    return tokens_PoS #restituisce la lista di tuple (token,PoS)\n",
    "\n",
    "frasi1, tokens1 = splitter_e_tokenizzatore(corpus1)\n",
    "tokens_PoS1 = PoS_tagger(tokens1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 PoS più frequenti (con relativa frequenza)\n",
      "NN________________________________________________892\n",
      "IN________________________________________________711\n",
      "DT________________________________________________576\n",
      "JJ________________________________________________566\n",
      "NNS_______________________________________________421\n",
      "NNP_______________________________________________357\n",
      ",_________________________________________________284\n",
      "RB________________________________________________235\n",
      "VB________________________________________________213\n",
      "._________________________________________________207\n",
      "\n",
      "10 bigrammi di PoS più frequenti (con relativa frequenza)\n",
      "('JJ', 'NN')______________________________________247\n",
      "('NN', 'IN')______________________________________229\n",
      "('IN', 'DT')______________________________________222\n",
      "('DT', 'NN')______________________________________209\n",
      "('DT', 'JJ')______________________________________164\n",
      "('JJ', 'NNS')_____________________________________128\n",
      "('IN', 'NN')______________________________________127\n",
      "('NNS', 'IN')_____________________________________110\n",
      "('NN', 'NN')______________________________________108\n",
      "('TO', 'VB')______________________________________101\n",
      "\n",
      "10 trigrammi di PoS più frequenti (con relativa frequenza)\n",
      "('DT', 'JJ', 'NN')________________________________110\n",
      "('DT', 'NN', 'IN')________________________________90\n",
      "('NN', 'IN', 'DT')________________________________68\n",
      "('IN', 'DT', 'NN')________________________________65\n",
      "('IN', 'DT', 'JJ')________________________________62\n",
      "('JJ', 'NN', 'IN')________________________________62\n",
      "('IN', 'DT', 'NNP')_______________________________50\n",
      "('NN', 'IN', 'NN')________________________________48\n",
      "('NN', 'IN', 'JJ')________________________________41\n",
      "('IN', 'JJ', 'NNS')_______________________________39\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "def stampa_lista_di_tuple(lista): #funzione ausiliaria che formatta la stampa in colonna di una lista di tuple costituite da 2 elementi\n",
    "    for i in range(len(lista)): #range(len(list)) crea una sequenza di numeri da 0 a len(list)-1\n",
    "        print(f\"{lista[i][0]}\".ljust(50,\"_\")+f\"{lista[i][1]}\") #il metodo .ljust() consente di giustificare a sinistra una stringa usando \"_\" come separatore\n",
    "\n",
    "def trova_ngrammiPoS_frequenti(tokens,n): #la funzione estrae i 10 n-grammi di PoS più frequenti\n",
    "    PoS_tags = [PoS for token,PoS in PoS_tagger(tokens)] #mediante la sintassi di list comprehension per ogni tupla (token,PoS) restituita dalla funzione \"PoS_tagger\" vengono estratti solo i PoS tags\n",
    "    if n==1: #se n=1 (unigrammi) calcola e ordina direttamente le frequenze dei tags in PoS_tags\n",
    "        ngrammi_PoS_frequenti = nltk.FreqDist(PoS_tags).most_common(10) #con \"FreqDist\" (funzione di libreria) si ottiene un dizionario di frequenze ordinato in maniera decrescente; con .most_common(10) si ottiene una lista dei primi 10 elementi\n",
    "    else: #se n!=1 (n-grammi) estrae prima gli n-grammi e poi calcola e ordina le frequenze\n",
    "        ngrammi_PoS = list(nltk.ngrams(PoS_tags, n)) #estrae n-grammi con il metodo nltk.ngrams()\n",
    "        ngrammi_PoS_frequenti = nltk.FreqDist(ngrammi_PoS).most_common(10)\n",
    "    return ngrammi_PoS_frequenti #restituisce la lista di 10 tuple (ngramma,frequenza) ordinate per frequenza decrescente\n",
    "\n",
    "print(\"10 PoS più frequenti (con relativa frequenza)\")\n",
    "stampa_lista_di_tuple(trova_ngrammiPoS_frequenti(tokens1,1))\n",
    "print(\"\")\n",
    "print(\"10 bigrammi di PoS più frequenti (con relativa frequenza)\")\n",
    "stampa_lista_di_tuple(trova_ngrammiPoS_frequenti(tokens1,2))\n",
    "print(\"\")\n",
    "print(\"10 trigrammi di PoS più frequenti (con relativa frequenza)\")\n",
    "stampa_lista_di_tuple(trova_ngrammiPoS_frequenti(tokens1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 Sostantivi più frequenti (con relativa frequenza)\n",
      "food______________________________________________43\n",
      "UK________________________________________________28\n",
      "NHS_______________________________________________21\n",
      "people____________________________________________20\n",
      "sector____________________________________________19\n",
      "China_____________________________________________19\n",
      "healthcare________________________________________18\n",
      "power_____________________________________________17\n",
      "government________________________________________16\n",
      "India_____________________________________________16\n",
      "immigration_______________________________________14\n",
      "migrants__________________________________________13\n",
      "charity___________________________________________13\n",
      "world_____________________________________________12\n",
      "workers___________________________________________10\n",
      "system____________________________________________10\n",
      "growth____________________________________________10\n",
      "right_____________________________________________9\n",
      "population________________________________________9\n",
      "United____________________________________________9\n",
      "\n",
      "20 Avverbi più frequenti (con relativa frequenza)\n",
      "not_______________________________________________30\n",
      "also______________________________________________16\n",
      "more______________________________________________10\n",
      "now_______________________________________________9\n",
      "only______________________________________________8\n",
      "highly____________________________________________6\n",
      "increasingly______________________________________6\n",
      "even______________________________________________5\n",
      "so________________________________________________5\n",
      "often_____________________________________________5\n",
      "just______________________________________________4\n",
      "n't_______________________________________________4\n",
      "further___________________________________________4\n",
      "very______________________________________________4\n",
      "then______________________________________________4\n",
      "already___________________________________________3\n",
      "most______________________________________________3\n",
      "well______________________________________________3\n",
      "namely____________________________________________3\n",
      "openly____________________________________________3\n",
      "\n",
      "20 Aggettivi più frequenti (con relativa frequenza)\n",
      "for-profit________________________________________22\n",
      "new_______________________________________________12\n",
      "other_____________________________________________11\n",
      "likely____________________________________________11\n",
      "national__________________________________________10\n",
      "global____________________________________________9\n",
      "local_____________________________________________9\n",
      "economic__________________________________________8\n",
      "international_____________________________________7\n",
      "due_______________________________________________7\n",
      "minimum___________________________________________7\n",
      "able______________________________________________7\n",
      "public____________________________________________7\n",
      "private___________________________________________7\n",
      "largest___________________________________________7\n",
      "subjective________________________________________7\n",
      "own_______________________________________________6\n",
      "positive__________________________________________6\n",
      "small_____________________________________________6\n",
      "social____________________________________________6\n"
     ]
    }
   ],
   "source": [
    "def trova_classePoS_frequenti(tokens_PoS,filtro_PoS): #la funzione estrae e stampa i 20 token più frequenti appartenenti a una certa classe di PoS dati la lista (token,PoS) e un filtro PoS\n",
    "    tokens_filtrati = [token for token,PoS in tokens_PoS if PoS.startswith(filtro_PoS)] #mediante la sintassi di list comprehension sono estratti i token appartenenti alla stessa classe di PoS filtrata con il metodo .startswith() perché a una PoS (es. Sostantivo) possono corrispondere più tags (es. \"NN\", \"NNP\", \"NNPS\", \"NNS\")\n",
    "    tokens_frequenti = nltk.FreqDist(tokens_filtrati).most_common(20) #con \"FreqDist\" (funzione di libreria) si ottiene un dizionario di frequenze ordinato in maniera decrescente; con .most_common(20) si ottiene una lista dei primi 20 elementi\n",
    "    return tokens_frequenti #restituisce la lista di 20 tuple (token,frequenza) ordinate per frequenza decrescente\n",
    "\n",
    "print(\"20 Sostantivi più frequenti (con relativa frequenza)\")\n",
    "stampa_lista_di_tuple(trova_classePoS_frequenti(tokens_PoS1,\"N\"))\n",
    "print(\"\")\n",
    "print(\"20 Avverbi più frequenti (con relativa frequenza)\")\n",
    "stampa_lista_di_tuple(trova_classePoS_frequenti(tokens_PoS1,\"R\"))\n",
    "print(\"\")\n",
    "print(\"20 Aggettivi più frequenti (con relativa frequenza)\")\n",
    "stampa_lista_di_tuple(trova_classePoS_frequenti(tokens_PoS1,\"J\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 20 bigrammi Aggettivo, Sostantivo più frequenti (con relativa frequenza)\n",
      "('for-profit', 'sector')__________________________9\n",
      "('minimum', 'service')____________________________7\n",
      "('for-profit', 'healthcare')______________________6\n",
      "('for-profit', 'companies')_______________________5\n",
      "('global', 'affairs')_____________________________5\n",
      "('subjective', 'well-being')______________________4\n",
      "('medical', 'consultants')________________________3\n",
      "('two-tier', 'system')____________________________3\n",
      "('new', 'balance')________________________________3\n",
      "('great', 'power')________________________________3\n",
      "('international', 'order')________________________3\n",
      "('sharp', 'polarisation')_________________________3\n",
      "('local', 'area')_________________________________3\n",
      "('national', 'identity')__________________________3\n",
      "('international', 'law')__________________________2\n",
      "('good', 'faith')_________________________________2\n",
      "('first', 'time')_________________________________2\n",
      "('disposable', 'incomes')_________________________2\n",
      "('collective', 'agreements')______________________2\n",
      "('collective', 'action')__________________________2\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def estrai_bigrammi_agg_sost(tokens): #la funzione estrae i bigrammi composti da aggettivo e sostantivo\n",
    "    bigrammi_token_PoS = list(nltk.bigrams(PoS_tagger(tokens))) #sono estratti i bigrammi di tuple (token,pos) ottenute dal PoS tagging effettuato sui tokens in input    \n",
    "    bigrammi_agg_sost = [(token1,token2) for (token1,PoS1),(token2,PoS2) in bigrammi_token_PoS if PoS1.startswith(\"J\") and PoS2.startswith(\"N\")] #mediante la sintassi di list comprehension sono estratti i bigrammi (token_aggettivo,token_sostantivo); è usato il metodo .startswith() perché a una PoS (es. Sostantivo) possono corrispondere più tags (es. \"NN\", \"NNP\", \"NNPS\", \"NNS\")\n",
    "    return bigrammi_agg_sost #restituisce la lista di tuple (token,token) di cui il primo è un aggettivo e il secondo è un sostantivo\n",
    "\n",
    "def trova_bigrammi_frequenti(bigrammi): #la funzione restituisce i bigrammi più frequenti data una lista di bigrammi\n",
    "    bigrammi_frequenti = nltk.FreqDist(bigrammi).most_common(20) #con \"FreqDist\" (funzione di libreria) si ottiene un dizionario di frequenze ordinato in maniera decrescente; con .most_common(20) si ottiene una lista dei primi 20 elementi\n",
    "    return bigrammi_frequenti #restituisce la lista delle prime 20 tuple (bigramma,frequenza) ordinate per frequenza decrescente\n",
    "\n",
    "print(\"I 20 bigrammi Aggettivo, Sostantivo più frequenti (con relativa frequenza)\")\n",
    "stampa_lista_di_tuple(trova_bigrammi_frequenti(estrai_bigrammi_agg_sost(tokens1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 20 bigrammi Aggettivo, Sostantivo con probabilità condizionata massima (e relativo valore)\n",
      "('medical', 'consultants')________________________1.0\n",
      "('sharp', 'polarisation')_________________________1.0\n",
      "('good', 'faith')_________________________________1.0\n",
      "('disposable', 'incomes')_________________________1.0\n",
      "('out-of-pocket', 'payments')_____________________1.0\n",
      "('expert', 'workforce')___________________________1.0\n",
      "('unknown', 'kind')_______________________________1.0\n",
      "('Western', 'world')______________________________1.0\n",
      "('Collective', 'agreements')______________________1.0\n",
      "('strict', 'safety')______________________________1.0\n",
      "('postal', 'ballots')_____________________________1.0\n",
      "('rich', 'countries')_____________________________1.0\n",
      "('independent', 'body')___________________________1.0\n",
      "('gross', 'violation')____________________________1.0\n",
      "('inalienable', 'rights')_________________________1.0\n",
      "('excess', 'profits')_____________________________1.0\n",
      "('reckless', 'management')________________________1.0\n",
      "('enterprising', 'culture')_______________________1.0\n",
      "('Good', 'employers')_____________________________1.0\n",
      "('mutual', 'respect')_____________________________1.0\n"
     ]
    }
   ],
   "source": [
    "def trova_bigrammi_probCondizionata_massima(bigrammi,tokens): #la funzione restituisce i bigrammi con Probabilità Condizionata massima data la lista di bigrammi e la lista di tokens\n",
    "    bigrammi_frequenza = nltk.FreqDist(bigrammi).most_common() #il metodo .most_common() privo di argomento restituisce la lista completa di tuple (bigramma,frequenza)\n",
    "    bigrammi_probabilita = {} #crea un dizionario per memorizzare la probabilità di ciascun bigramma\n",
    "    for bigramma,frequenza in bigrammi_frequenza: #si scorre la lista \"bigrammi_frequenza\"\n",
    "        frequenza_parola1 = tokens.count(bigramma[0]) #si ottiene la frequenza della prima parola del bigramma (che è una tupla (token,token))\n",
    "        probabilita_bigramma = frequenza/frequenza_parola1 #calcola la probabilità condizionata del bigramma\n",
    "        bigrammi_probabilita.update({bigramma:probabilita_bigramma}) #aggiorna il dizionario con il nuovo valore {bigramma:probabilità}\n",
    "    bigrammi_probabilita_decrescente = sorted(bigrammi_probabilita.items(), key=lambda x:x[1], reverse=True) #a partire dal dizionario di ottiene una lista ordinata per valori decrescenti con la funzione di libreria \"sorted\"\n",
    "    return bigrammi_probabilita_decrescente[:20] #si stampano i primi 20 elementi della lista con lo slicing\n",
    "\n",
    "print(\"I 20 bigrammi Aggettivo, Sostantivo con probabilità condizionata massima (e relativo valore)\")\n",
    "stampa_lista_di_tuple(trova_bigrammi_probCondizionata_massima(estrai_bigrammi_agg_sost(tokens1),tokens1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 20 bigrammi Aggettivo, Sostantivo con Pointwise Mutual Information massima (e relativo valore)\n",
      "('unknown', 'kind')_______________________________12.566\n",
      "('strict', 'safety')______________________________12.566\n",
      "('postal', 'ballots')_____________________________12.566\n",
      "('independent', 'body')___________________________12.566\n",
      "('gross', 'violation')____________________________12.566\n",
      "('excess', 'profits')_____________________________12.566\n",
      "('mutual', 'respect')_____________________________12.566\n",
      "('dominant', 'provider')__________________________12.566\n",
      "('biggest', 'earner')_____________________________12.566\n",
      "('generous', 'profit')____________________________12.566\n",
      "('managed', 'decline')____________________________12.566\n",
      "('second', 'constraint')__________________________12.566\n",
      "('post-war', 'welfare')___________________________12.566\n",
      "('twenty', 'richest')_____________________________12.566\n",
      "('world-wide', 'recession')_______________________12.566\n",
      "('closer', 'links')_______________________________12.566\n",
      "('multilateral', 'regimes')_______________________12.566\n",
      "('domestic', 'interference')______________________12.566\n",
      "('imminent', 'attempt')___________________________12.566\n",
      "('golden', 'era')_________________________________12.566\n"
     ]
    }
   ],
   "source": [
    "def trova_bigrammi_PMI_massima(bigrammi,tokens): #la funzione restituisce i bigrammi con Pointwise Mutual Information massima data la lista di bigrammi e la lista di tokens\n",
    "    bigrammi_frequenza = nltk.FreqDist(bigrammi).most_common() #il metodo .most_common() privo di argomento restituisce la lista completa di tuple (bigramma,frequenza)\n",
    "    bigrammi_PMI = {} #crea un dizionario per memorizzare la PMI di ciascun bigramma\n",
    "    for bigramma,frequenza in bigrammi_frequenza: #si scorre la lista \"bigrammi_frequenza\"\n",
    "        frequenza_parola1 = tokens.count(bigramma[0]) #si ottiene la frequenza della prima parola del bigramma (che è una tupla (token,token))\n",
    "        frequenza_parola2 = tokens.count(bigramma[1]) #si ottiene la frequenza della seconda parola del bigramma\n",
    "        PMI_bigramma = round(math.log2((frequenza*len(tokens1))/(frequenza_parola1*frequenza_parola2)),3) #calcola la PMI del bigramma (NB: tokens1 è una variabile globale)\n",
    "        bigrammi_PMI.update({bigramma:PMI_bigramma}) #aggiorna il dizionario con il nuovo valore {bigramma:PMI_bigramma}\n",
    "    bigrammi_PMI_decrescente = sorted(bigrammi_PMI.items(), key=lambda x:x[1], reverse=True) #a partire dal dizionario di ottiene una lista ordinata per valori decrescenti con la funzione di libreria \"sorted\"\n",
    "    return bigrammi_PMI_decrescente[:20] #si stampano i primi 20 elementi della lista con lo slicing\n",
    "\n",
    "print(\"I 20 bigrammi Aggettivo, Sostantivo con Pointwise Mutual Information massima (e relativo valore)\")\n",
    "stampa_lista_di_tuple(trova_bigrammi_PMI_massima(estrai_bigrammi_agg_sost(tokens1),tokens1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ottieni_frasi_tokenizzate(frasi): #funzione ausiliaria che restituisce un dizionario di frasi tokenizzate\n",
    "    frasi_tokenizzate = {}\n",
    "    for frase in frasi: #si scorre la lista di frasi in input\n",
    "        tokens_frase = nltk.tokenize.word_tokenize(frase) #tokenizza ciascuna frase\n",
    "        frasi_tokenizzate.update({frase:tokens_frase}) #il dizionario è aggiornato con la nuova coppia {chiave:valore}\n",
    "    return frasi_tokenizzate #restituisce un dizionario in cui le chiavi sono le frasi originarie e i valori sono le frasi tokenizzate individualmente\n",
    "\n",
    "def trova_hapax(tokens): #funzione ausiliaria che restituisce in output la lista di hapax\n",
    "    hapax = [token for token in list(set(tokens)) if tokens.count(token) == 1] #mediante la sintassi di list comprehension si estraggono i token che ricorrono solo 1 volta; si usa list(set(tokens)) per ottenere il vocabolario di parole tipo\n",
    "    return hapax\n",
    "\n",
    "def seleziona_frasi_10_20(corpus): #la funzione seleziona le frasi con lunghezza compresa tra 10 e 20 token, di cui la metà NON sono hapax\n",
    "    frasi,tokens = splitter_e_tokenizzatore(corpus) #estrae frasi e tokens dal corpus\n",
    "    frasi_tokenizzate = ottieni_frasi_tokenizzate(frasi) #ottiene il dizionario di frasi tokenizzate\n",
    "    hapax = trova_hapax(tokens) #ottiene la lista di hapax del corpus\n",
    "    frasi_selezionate = []\n",
    "    for frase,tokens_frase in frasi_tokenizzate.items(): #si scorre il dizionario\n",
    "        if len(tokens_frase)>=10 and len(tokens_frase)<=20: #vengono prese in considerazione solo le frasi la cui lunghezza è compresa tra 10 e 20 token\n",
    "            i = 0 #si inizializza a 0 una variabile i (contatore) che tiene il conto dei token che NON sono hapax\n",
    "            for token in tokens_frase: #si scorre la lista di tokens (di ciascuna frase)\n",
    "                if token not in hapax: #se il token NON è presente nella lista di hapax il contatore si aggiorna\n",
    "                    i += 1\n",
    "            if i >= math.floor(len(tokens_frase)/2): #se il valore del contatore è almeno pari alla metà della lunghezza della frase allora la frase viene aggiunta alla lista \"frasi_selezionate\"\n",
    "                frasi_selezionate.append(frase)\n",
    "    return frasi_selezionate #restituisce la lista contenente le frasi selezionate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La frase con la media della distribuzione di frequenza dei token più alta, ovvero 100.154, è:\n",
      "\tImmigration fears, for example, dominated much of the Brexit debate.\n",
      "La frase con la media della distribuzione di frequenza dei token più bassa, ovvero 20.214, è:\n",
      "\tSuch an outcome did however unveil some increasingly familiar dynamics within global affairs.\n"
     ]
    }
   ],
   "source": [
    "def media_distrFreq_max_min(frasi,tokens): #la funzione calcola la media della distribuzione di frequenza dei token per ogni frase della lista in input e restituisce le frasi con il valore massimo e minimo\n",
    "    frasi_tokenizzate = ottieni_frasi_tokenizzate(frasi) #si ottiene il dizionario di frasi tokenizzate\n",
    "    max = 0 #si inizializza a 0 il valore massimo\n",
    "    min = float(\"inf\") #si inizializza a infinito il valore minimo\n",
    "    for frase,tokens_frase in frasi_tokenizzate.items(): #si scorre il dizionario\n",
    "        somma = 0 #si inizializza a 0 la variabile che contiene la somma\n",
    "        for token in tokens_frase: #si scorre la lista di tokens (per ciascuna frase)\n",
    "            frequenza_token = tokens.count(token) #si ottiene la frequenza del token nell'intero corpus\n",
    "            somma += frequenza_token #somma le frequenze di ciascun token della frase\n",
    "        media_distrFreq_frase = round(somma/len(tokens_frase),3) #calcola la media della distribuzione di frequenza dei token per ogni frase, ovvero: si divide il totale della frequenza di tutti i token per il numero di token (il valore è arrotondato a 3 cifre decimali)\n",
    "        \n",
    "        if media_distrFreq_frase > max: #se il valore è maggiore del valore massimo, quest'ultimo viene aggiornato\n",
    "            max = media_distrFreq_frase\n",
    "            frase_max = frase #memorizza la frase con il valore massimo\n",
    "\n",
    "        if media_distrFreq_frase < min: #se il valore è minore del valore minimo, quest'ultimo viene aggiornato\n",
    "            min = media_distrFreq_frase\n",
    "            frase_min = frase #memorizza la frase con il valore minimo\n",
    "\n",
    "    return frase_max, max, frase_min, min\n",
    "\n",
    "frase_max1,max1,frase_min1,min1 = media_distrFreq_max_min(seleziona_frasi_10_20(corpus1),tokens1)\n",
    "print(f\"La frase con la media della distribuzione di frequenza dei token più alta, ovvero {max1}, è:\\n\\t{frase_max1}\")\n",
    "print(f\"La frase con la media della distribuzione di frequenza dei token più bassa, ovvero {min1}, è:\\n\\t{frase_min1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La frase con la probabilità più alta secondo il modello di Markov di ordine 2, ovvero 0.0001649620587264929, è:\n",
      "\tOf course, one can easily understand negative sentiments here.\n"
     ]
    }
   ],
   "source": [
    "def markov2_max(frasi,tokens): #la funzione calcola la probabilità delle frasi secondo un modello di Markov di ordine 2 e restituisce quella con probabilità massima \n",
    "    frasi_tokenizzate = ottieni_frasi_tokenizzate(frasi) #si ottiene il dizionario di frasi tokenizzate\n",
    "    bigrammi_corpus = list(nltk.bigrams(tokens)) #estrae i bigrammi di token dall'intero corpus\n",
    "    trigrammi_corpus = list(nltk.trigrams(tokens)) #estrae i trigrammi di token dall'intero corpus\n",
    "    max = 0 #si inizializza a 0 il valore massimo\n",
    "\n",
    "    for frase,tokens_frase in frasi_tokenizzate.items(): #si scorre il dizionario\n",
    "        bigrammi_frase = list(nltk.bigrams(tokens_frase)) #estrae i bigrammi di token dalla frase\n",
    "        trigrammi_frase = list(nltk.trigrams(tokens_frase)) #estrae i trigrammi di token dalla frase\n",
    "\n",
    "        prob_parola1 = tokens.count(tokens_frase[0])/len(tokens) #si calcola la probabilità della prima parola della catena di Markov\n",
    "        prob_parola2 = (bigrammi_corpus.count(bigrammi_frase[0]))/tokens.count(tokens_frase[0]) #si calcola la probabilità della seconda parola della catena di Markov\n",
    "        prodotto = prob_parola1 * prob_parola2 #si inizializza una variabile che dovrà contenere il prodotto complessivo della catena di Markov\n",
    "        \n",
    "        for i in range(len(trigrammi_frase)): #si scorrono i trigrammi della frase\n",
    "            frequenza_trigramma = trigrammi_corpus.count(trigrammi_frase[i]) #estrae la frequenza del trigramma\n",
    "            frequenza_bigramma = bigrammi_corpus.count(bigrammi_frase[i]) #estrae la frequenza del bigramma\n",
    "            prob_condizionata_parola = frequenza_trigramma/frequenza_bigramma #calcola la probabilità condizionata della seguente parola (dalla terza in poi) nella catena di Markov\n",
    "            prodotto *= prob_condizionata_parola #il valore è moltiplicato al prodotto complessivo\n",
    "        \n",
    "        if prodotto > max: #se il prodotto complessivo è maggiore del valore massimo, quest'ultimo si aggiorna\n",
    "            max = prodotto\n",
    "            frase_max = frase #si memorizza la frase corrispondete al prodotto massimo\n",
    "            \n",
    "    return frase_max, max\n",
    "\n",
    "frase_max_markov, max_markov = markov2_max(seleziona_frasi_10_20(corpus1),tokens1)\n",
    "print(f\"La frase con la probabilità più alta secondo il modello di Markov di ordine 2, ovvero {max_markov}, è:\\n\\t{frase_max_markov}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPE:\n",
      "China_____________________________________________19\n",
      "India_____________________________________________16\n",
      "British___________________________________________8\n",
      "Russia____________________________________________7\n",
      "United States_____________________________________7\n",
      "English___________________________________________7\n",
      "France____________________________________________4\n",
      "Germany___________________________________________4\n",
      "Beijing___________________________________________4\n",
      "London____________________________________________3\n",
      "Ukraine___________________________________________3\n",
      "Japan_____________________________________________3\n",
      "Italy_____________________________________________2\n",
      "Spain_____________________________________________2\n",
      "French____________________________________________2\n",
      "\n",
      "ORGANIZATION:\n",
      "UK________________________________________________21\n",
      "NHS_______________________________________________19\n",
      "Trussell Trust____________________________________5\n",
      "ILO_______________________________________________4\n",
      "Parliament________________________________________2\n",
      "G20_______________________________________________2\n",
      "United Kingdom____________________________________2\n",
      "General Health Questionnaire______________________2\n",
      "FareShare_________________________________________2\n",
      "Minimum Service Levels____________________________1\n",
      "Business__________________________________________1\n",
      "International Labour Organisation_________________1\n",
      "Labour Code_______________________________________1\n",
      "Constitutional Court______________________________1\n",
      "Universal_________________________________________1\n",
      "\n",
      "PERSON:\n",
      "Strikes___________________________________________4\n",
      "Sunak_____________________________________________3\n",
      "Xi________________________________________________3\n",
      "Bill______________________________________________2\n",
      "Grant Shapps______________________________________2\n",
      "Shapps Bill_______________________________________2\n",
      "Shapps____________________________________________2\n",
      "Xi Jinping________________________________________2\n",
      "Tony Blair________________________________________1\n",
      "Europe Earlier____________________________________1\n",
      "Rishi Sunak_______________________________________1\n",
      "Transport Code____________________________________1\n",
      "Law No____________________________________________1\n",
      "Apartheid_________________________________________1\n",
      "Viktor Orbán______________________________________1\n",
      "\n",
      "LOCATION:\n",
      "Western___________________________________________1\n",
      "West______________________________________________1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def ottieni_NE(tokens_PoS): #la funzione ottiene un dizionario di NE a partire dalla lista di tuple (token,PoS)\n",
    "    albero_NE = nltk.ne_chunk(tokens_PoS) #si genera l'albero delle Named Entities a partire dalla lista di tuple (token,PoS)\n",
    "    NE = {} #viene creato un dizionario vuoto\n",
    "    for nodo in albero_NE: #si scorre l'albero attraverso i nodi\n",
    "        if hasattr(nodo, 'label'): #se il nodo ha attributo 'label' si accede all'attributo label del nodo, che contiene la classe dell'entità\n",
    "            classe = nodo.label() #memorizza il nome della classe\n",
    "            entita = \" \".join([token for token,PoS in nodo.leaves()]) #ricostruisce la stringa completa dell'entità\n",
    "            if classe in NE.keys(): #se \"classe\" è tra le chiavi del dizionario NE allora la lista di entità corrispondenti viene aggiornata con il nuovo elemento\n",
    "                NE[classe].append(entita)\n",
    "            else: #altrimenti \"classe\" diventa una nuova chiave del dizionario\n",
    "                NE[classe] = [entita]\n",
    "    return NE #restituisce un dizionario in cui le chiavi sono le classi di NE e i valori sono liste contenenti i tokens appartenenti alla classe\n",
    "\n",
    "def trova_NE_frequenti(NE,classe): #la funzione restituisce gli elementi più frequenti (con relativa frequenza) per una classe di NE\n",
    "    FreqDist_classe_NE = FreqDist(NE[classe]).most_common(15)\n",
    "    return FreqDist_classe_NE #restituisce la lista delle prime 15 tuple (entità,frequenza) ordinate per frequenza decrescente\n",
    "\n",
    "for classe in ottieni_NE(tokens_PoS1).keys(): #si scorrono le chiavi del dizionario NE e per ogni classe di NE si stampano le entità più frequenti trovate\n",
    "    print(f\"{classe}:\")\n",
    "    stampa_lista_di_tuple(trova_NE_frequenti(ottieni_NE(tokens_PoS1),classe))\n",
    "    print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7 (v3.10.7:6cc6b13308, Sep  5 2022, 14:02:52) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
