{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confronto tra ./LSEblog.txt e ./janeausten.txt \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def leggi_contenuto(file): #la funzione restituisce in output una variabile che ha memorizzato i contenuti del file con il metodo .read()\n",
    "    with open(file, \"r\") as infile:\n",
    "        contenuto = infile.read() #il metodo .read() consente di memorizzare il contenuto del file in una variabile\n",
    "    return contenuto\n",
    "\n",
    "file1 = \"./LSEblog.txt\"\n",
    "file2 = \"./janeausten.txt\"\n",
    "corpus1 = leggi_contenuto(file1)\n",
    "corpus2 = leggi_contenuto(file2)\n",
    "\n",
    "print(f\"Confronto tra {file1} e {file2} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Numero di frasi e di token:\n",
      "\t./LSEblog.txt possiede 207 frasi e 6062 tokens\n",
      "\t./janeausten.txt possiede 229 frasi e 6827 tokens\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "def splitter_e_tokenizzatore(corpus): #la funzione effettua il sentence splitting e la tokenizzazione del corpus\n",
    "    frasi = nltk.tokenize.sent_tokenize(corpus) #splitta il testo in frasi\n",
    "    tokens = []\n",
    "    for frase in frasi:\n",
    "        tokens_frase = nltk.tokenize.word_tokenize(frase) #tokenizza ciascuna frase\n",
    "        tokens.extend(tokens_frase) # .extend() aggiunge in coda alla lista di tokens del corpus i tokens della frase\n",
    "    return frasi, tokens\n",
    "\n",
    "def PoS_tagger(tokens): #la funzione effettua il PoS tagging data la lista di tokens del corpus\n",
    "    tokens_PoS = nltk.tag.pos_tag(tokens)\n",
    "    return tokens_PoS #restituisce la lista di tuple (token,PoS)\n",
    "\n",
    "frasi1, tokens1 = splitter_e_tokenizzatore(corpus1)\n",
    "frasi2, tokens2 = splitter_e_tokenizzatore(corpus2)\n",
    "tokens_PoS1 = PoS_tagger(tokens1)\n",
    "tokens_PoS2 = PoS_tagger(tokens2)\n",
    "\n",
    "print(\"1. Numero di frasi e di token:\")\n",
    "print(f\"\\t{file1} possiede {len(frasi1)} frasi e {len(tokens1)} tokens\")\n",
    "print(f\"\\t{file2} possiede {len(frasi2)} frasi e {len(tokens2)} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Lunghezza media delle frasi in termini di token e dei token (a eccezione della punteggiatura) in termini di caratteri:\n",
      "\tOgni frase di ./LSEblog.txt è lunga in media 29 tokens e ogni token è lungo in media 5 caratteri\n",
      "\tOgni frase di ./janeausten.txt è lunga in media 30 tokens e ogni token è lungo in media 4 caratteri\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def lunghezza_media_frasi(corpus): #la funzione restituisce in output il valore della lunghezza media delle frasi del corpus in input\n",
    "    frasi, tokens = splitter_e_tokenizzatore(corpus)\n",
    "    return round(len(tokens)/len(frasi)) #si approssima senza decimali perché i token non sono divisibili!\n",
    "\n",
    "def rimuovi_punteggiatura(tokens): #la funzione restituisce in output la lista di token esclusa la punteggiatura\n",
    "    tokens_no_punt = [token for token in tokens if re.match(r'\\b[\\w]+\\b', token)] #mediante la sintassi di list comprehension si estraggono i token che corrispondono alla RE \"\\b[\\w]+\\b\"\n",
    "    return tokens_no_punt\n",
    "\n",
    "def lunghezza_media_tokens(tokens): #la funzione restituisce in output la lunghezza media dei token data una lista di token\n",
    "    tot_characters = 0\n",
    "    for token in tokens:\n",
    "        tot_characters += len(token)\n",
    "    return round(tot_characters/len(tokens))\n",
    "\n",
    "lunghezza_media_frasi1 = lunghezza_media_frasi(corpus1)\n",
    "lunghezza_media_frasi2 = lunghezza_media_frasi(corpus2)\n",
    "\n",
    "lunghezza_media_tokens1 = lunghezza_media_tokens(rimuovi_punteggiatura(tokens1))\n",
    "lunghezza_media_tokens2 = lunghezza_media_tokens(rimuovi_punteggiatura(tokens2))\n",
    "\n",
    "print(\"2. Lunghezza media delle frasi in termini di token e dei token (a eccezione della punteggiatura) in termini di caratteri:\")\n",
    "print(f\"\\tOgni frase di {file1} è lunga in media {lunghezza_media_frasi1} tokens e ogni token è lungo in media {lunghezza_media_tokens1} caratteri\")\n",
    "print(f\"\\tOgni frase di {file2} è lunga in media {lunghezza_media_frasi2} tokens e ogni token è lungo in media {lunghezza_media_tokens2} caratteri\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. Numero di Hapax tra i primi 500, 1000, 3000 token e nell’intero corpus:\n",
      "\t./LSEblog.txt ha un totale di:\n",
      "\t\t174 hapax nei primi 500 tokens del corpus.\n",
      "\t\t289 hapax nei primi 1000 tokens del corpus.\n",
      "\t\t679 hapax nei primi 3000 tokens del corpus.\n",
      "\t\t1094 hapax nell'intero corpus.\n",
      "\t./janeausten.txt ha un totale di:\n",
      "\t\t183 hapax nei primi 500 tokens del corpus.\n",
      "\t\t287 hapax nei primi 1000 tokens del corpus.\n",
      "\t\t448 hapax nei primi 3000 tokens del corpus.\n",
      "\t\t850 hapax nell'intero corpus.\n"
     ]
    }
   ],
   "source": [
    "def trova_hapax(tokens): #la funzione restituisce in output la lista di hapax\n",
    "    hapax = [token for token in list(set(tokens)) if tokens.count(token) == 1] #mediante la sintassi di list comprehension si estraggono i token che ricorrono solo 1 volta; si usa list(set(tokens)) per ottenere il vocabolario di parole tipo\n",
    "    return hapax\n",
    "\n",
    "def ottieni_slice(tokens,limite): #la funzione restituisce in output la sottolista che parte dal primo elemento della lista \"tokens\" in input fino al valore \"limite\"\n",
    "    tokens_slice = tokens[:limite] #crea sottolista che parte dal primo elemento di \"tokens\" fino al valore \"limite\"\n",
    "    return tokens_slice\n",
    "\n",
    "limiti = [500,1000,3000]\n",
    "\n",
    "print(f\"3. Numero di Hapax tra i primi {limiti[0]}, {limiti[1]}, {limiti[2]} token e nell’intero corpus:\")\n",
    "print(f\"\\t{file1} ha un totale di:\")\n",
    "for x in limiti:\n",
    "    print(f\"\\t\\t{len(trova_hapax(ottieni_slice(tokens1,x)))} hapax nei primi {x} tokens del corpus.\")\n",
    "print(f\"\\t\\t{len(trova_hapax(tokens1))} hapax nell'intero corpus.\")\n",
    "    \n",
    "print(f\"\\t{file2} ha un totale di:\")\n",
    "for x in limiti:\n",
    "    print(f\"\\t\\t{len(trova_hapax(ottieni_slice(tokens2,x)))} hapax nei primi {x} tokens del corpus.\")\n",
    "print(f\"\\t\\t{len(trova_hapax(tokens2))} hapax nell'intero corpus.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. Dimensione del vocabolario e ricchezza lessicale calcolata attraverso la TTR per porzioni incrementali di 200 token (i.e., i primi 200, i primi 400, i primi 600, ...):\n",
      "\t./LSEblog.txt:\n",
      "\t\tper i primi 200 tokens, il vocabolario è lungo 135 types e l'indice di ricchezza lessicale (TTR) è 0.675\n",
      "\t\tper i primi 400 tokens, il vocabolario è lungo 218 types e l'indice di ricchezza lessicale (TTR) è 0.545\n",
      "\t\tper i primi 600 tokens, il vocabolario è lungo 285 types e l'indice di ricchezza lessicale (TTR) è 0.475\n",
      "\t\tper i primi 800 tokens, il vocabolario è lungo 346 types e l'indice di ricchezza lessicale (TTR) è 0.432\n",
      "\t\tper i primi 1000 tokens, il vocabolario è lungo 420 types e l'indice di ricchezza lessicale (TTR) è 0.42\n",
      "\t\tper i primi 1200 tokens, il vocabolario è lungo 491 types e l'indice di ricchezza lessicale (TTR) è 0.409\n",
      "\t\tper i primi 1400 tokens, il vocabolario è lungo 554 types e l'indice di ricchezza lessicale (TTR) è 0.396\n",
      "\t\tper i primi 1600 tokens, il vocabolario è lungo 621 types e l'indice di ricchezza lessicale (TTR) è 0.388\n",
      "\t\tper i primi 1800 tokens, il vocabolario è lungo 675 types e l'indice di ricchezza lessicale (TTR) è 0.375\n",
      "\t\tper i primi 2000 tokens, il vocabolario è lungo 730 types e l'indice di ricchezza lessicale (TTR) è 0.365\n",
      "\t\tper i primi 2200 tokens, il vocabolario è lungo 781 types e l'indice di ricchezza lessicale (TTR) è 0.355\n",
      "\t\tper i primi 2400 tokens, il vocabolario è lungo 849 types e l'indice di ricchezza lessicale (TTR) è 0.354\n",
      "\t\tper i primi 2600 tokens, il vocabolario è lungo 920 types e l'indice di ricchezza lessicale (TTR) è 0.354\n",
      "\t\tper i primi 2800 tokens, il vocabolario è lungo 986 types e l'indice di ricchezza lessicale (TTR) è 0.352\n",
      "\t\tper i primi 3000 tokens, il vocabolario è lungo 1040 types e l'indice di ricchezza lessicale (TTR) è 0.347\n",
      "\t\tper i primi 3200 tokens, il vocabolario è lungo 1084 types e l'indice di ricchezza lessicale (TTR) è 0.339\n",
      "\t\tper i primi 3400 tokens, il vocabolario è lungo 1131 types e l'indice di ricchezza lessicale (TTR) è 0.333\n",
      "\t\tper i primi 3600 tokens, il vocabolario è lungo 1168 types e l'indice di ricchezza lessicale (TTR) è 0.324\n",
      "\t\tper i primi 3800 tokens, il vocabolario è lungo 1227 types e l'indice di ricchezza lessicale (TTR) è 0.323\n",
      "\t\tper i primi 4000 tokens, il vocabolario è lungo 1282 types e l'indice di ricchezza lessicale (TTR) è 0.321\n",
      "\t\tper i primi 4200 tokens, il vocabolario è lungo 1333 types e l'indice di ricchezza lessicale (TTR) è 0.317\n",
      "\t\tper i primi 4400 tokens, il vocabolario è lungo 1386 types e l'indice di ricchezza lessicale (TTR) è 0.315\n",
      "\t\tper i primi 4600 tokens, il vocabolario è lungo 1417 types e l'indice di ricchezza lessicale (TTR) è 0.308\n",
      "\t\tper i primi 4800 tokens, il vocabolario è lungo 1462 types e l'indice di ricchezza lessicale (TTR) è 0.305\n",
      "\t\tper i primi 5000 tokens, il vocabolario è lungo 1504 types e l'indice di ricchezza lessicale (TTR) è 0.301\n",
      "\t\tper i primi 5200 tokens, il vocabolario è lungo 1554 types e l'indice di ricchezza lessicale (TTR) è 0.299\n",
      "\t\tper i primi 5400 tokens, il vocabolario è lungo 1604 types e l'indice di ricchezza lessicale (TTR) è 0.297\n",
      "\t\tper i primi 5600 tokens, il vocabolario è lungo 1659 types e l'indice di ricchezza lessicale (TTR) è 0.296\n",
      "\t\tper i primi 5800 tokens, il vocabolario è lungo 1707 types e l'indice di ricchezza lessicale (TTR) è 0.294\n",
      "\t\tper i primi 6000 tokens, il vocabolario è lungo 1752 types e l'indice di ricchezza lessicale (TTR) è 0.292\n",
      "\t\tper i primi 6062 tokens, il vocabolario è lungo 1763 types e l'indice di ricchezza lessicale (TTR) è 0.291\n",
      "\n",
      "\t./janeausten.txt:\n",
      "\t\tper i primi 200 tokens, il vocabolario è lungo 111 types e l'indice di ricchezza lessicale (TTR) è 0.555\n",
      "\t\tper i primi 400 tokens, il vocabolario è lungo 200 types e l'indice di ricchezza lessicale (TTR) è 0.5\n",
      "\t\tper i primi 600 tokens, il vocabolario è lungo 281 types e l'indice di ricchezza lessicale (TTR) è 0.468\n",
      "\t\tper i primi 800 tokens, il vocabolario è lungo 343 types e l'indice di ricchezza lessicale (TTR) è 0.429\n",
      "\t\tper i primi 1000 tokens, il vocabolario è lungo 406 types e l'indice di ricchezza lessicale (TTR) è 0.406\n",
      "\t\tper i primi 1200 tokens, il vocabolario è lungo 454 types e l'indice di ricchezza lessicale (TTR) è 0.378\n",
      "\t\tper i primi 1400 tokens, il vocabolario è lungo 503 types e l'indice di ricchezza lessicale (TTR) è 0.359\n",
      "\t\tper i primi 1600 tokens, il vocabolario è lungo 550 types e l'indice di ricchezza lessicale (TTR) è 0.344\n",
      "\t\tper i primi 1800 tokens, il vocabolario è lungo 584 types e l'indice di ricchezza lessicale (TTR) è 0.324\n",
      "\t\tper i primi 2000 tokens, il vocabolario è lungo 624 types e l'indice di ricchezza lessicale (TTR) è 0.312\n",
      "\t\tper i primi 2200 tokens, il vocabolario è lungo 669 types e l'indice di ricchezza lessicale (TTR) è 0.304\n",
      "\t\tper i primi 2400 tokens, il vocabolario è lungo 700 types e l'indice di ricchezza lessicale (TTR) è 0.292\n",
      "\t\tper i primi 2600 tokens, il vocabolario è lungo 725 types e l'indice di ricchezza lessicale (TTR) è 0.279\n",
      "\t\tper i primi 2800 tokens, il vocabolario è lungo 746 types e l'indice di ricchezza lessicale (TTR) è 0.266\n",
      "\t\tper i primi 3000 tokens, il vocabolario è lungo 769 types e l'indice di ricchezza lessicale (TTR) è 0.256\n",
      "\t\tper i primi 3200 tokens, il vocabolario è lungo 805 types e l'indice di ricchezza lessicale (TTR) è 0.252\n",
      "\t\tper i primi 3400 tokens, il vocabolario è lungo 835 types e l'indice di ricchezza lessicale (TTR) è 0.246\n",
      "\t\tper i primi 3600 tokens, il vocabolario è lungo 868 types e l'indice di ricchezza lessicale (TTR) è 0.241\n",
      "\t\tper i primi 3800 tokens, il vocabolario è lungo 888 types e l'indice di ricchezza lessicale (TTR) è 0.234\n",
      "\t\tper i primi 4000 tokens, il vocabolario è lungo 924 types e l'indice di ricchezza lessicale (TTR) è 0.231\n",
      "\t\tper i primi 4200 tokens, il vocabolario è lungo 989 types e l'indice di ricchezza lessicale (TTR) è 0.235\n",
      "\t\tper i primi 4400 tokens, il vocabolario è lungo 1039 types e l'indice di ricchezza lessicale (TTR) è 0.236\n",
      "\t\tper i primi 4600 tokens, il vocabolario è lungo 1084 types e l'indice di ricchezza lessicale (TTR) è 0.236\n",
      "\t\tper i primi 4800 tokens, il vocabolario è lungo 1119 types e l'indice di ricchezza lessicale (TTR) è 0.233\n",
      "\t\tper i primi 5000 tokens, il vocabolario è lungo 1153 types e l'indice di ricchezza lessicale (TTR) è 0.231\n",
      "\t\tper i primi 5200 tokens, il vocabolario è lungo 1180 types e l'indice di ricchezza lessicale (TTR) è 0.227\n",
      "\t\tper i primi 5400 tokens, il vocabolario è lungo 1218 types e l'indice di ricchezza lessicale (TTR) è 0.226\n",
      "\t\tper i primi 5600 tokens, il vocabolario è lungo 1254 types e l'indice di ricchezza lessicale (TTR) è 0.224\n",
      "\t\tper i primi 5800 tokens, il vocabolario è lungo 1275 types e l'indice di ricchezza lessicale (TTR) è 0.22\n",
      "\t\tper i primi 6000 tokens, il vocabolario è lungo 1304 types e l'indice di ricchezza lessicale (TTR) è 0.217\n",
      "\t\tper i primi 6200 tokens, il vocabolario è lungo 1333 types e l'indice di ricchezza lessicale (TTR) è 0.215\n",
      "\t\tper i primi 6400 tokens, il vocabolario è lungo 1373 types e l'indice di ricchezza lessicale (TTR) è 0.215\n",
      "\t\tper i primi 6600 tokens, il vocabolario è lungo 1403 types e l'indice di ricchezza lessicale (TTR) è 0.213\n",
      "\t\tper i primi 6800 tokens, il vocabolario è lungo 1438 types e l'indice di ricchezza lessicale (TTR) è 0.211\n",
      "\t\tper i primi 6827 tokens, il vocabolario è lungo 1445 types e l'indice di ricchezza lessicale (TTR) è 0.212\n"
     ]
    }
   ],
   "source": [
    "def analisi_incrementale(tokens,incremento): #la funzione stampa il resoconto su vocabolario e TTR per porzioni incrementali di token\n",
    "    base = 0 #si inizializza a 0 un contatore\n",
    "    while base < len(tokens): #fino a quando \"base\" rimane minore del numero totale di token si stampa l'analisi\n",
    "        if base+incremento > len(tokens): #se la somma di \"base\" e \"incremento\" è maggiore della lunghezza dei tokens allora \"base\" è aggiornata alla lunghezza dei tokens\n",
    "            base = len(tokens)\n",
    "        else: #altrimenti \"base\" è incrementata\n",
    "            base+=incremento\n",
    "        vocabolario_slice = list(set(ottieni_slice(tokens,base))) #si ottiene lo slice del vocabolario\n",
    "        corpus_slice = ottieni_slice(tokens,base) #si ottiene lo slice del corpus\n",
    "        ttr = round((len(vocabolario_slice)/len(corpus_slice)), 3) #si calcola la TTR (il valore è arrotondato a 3 cifre decimali)\n",
    "        print(f\"\\t\\tper i primi {len(corpus_slice)} tokens, il vocabolario è lungo {len(vocabolario_slice)} types e l'indice di ricchezza lessicale (TTR) è {ttr}\")\n",
    "\n",
    "print(f\"4. Dimensione del vocabolario e ricchezza lessicale calcolata attraverso la TTR per porzioni incrementali di 200 token (i.e., i primi 200, i primi 400, i primi 600, ...):\")\n",
    "print(f\"\\t{file1}:\")\n",
    "analisi_incrementale(tokens1,200)\n",
    "print(\"\")\n",
    "print(f\"\\t{file2}:\")\n",
    "analisi_incrementale(tokens2,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. Numero di lemmi distinti (i.e., la dimensione del vocabolario dei lemmi:\n",
      "\tIl vocabolario del ./LSEblog.txt ha un totale di 1533 lemmi.\n",
      "\tIl vocabolario del ./janeausten.txt ha un totale di 1267 lemmi.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def converti_PoS_in_wordnet(tag): #la funzione converte i tags del Penn Treebank in quelli di WordNet (funzionali al Lemmatizer)\n",
    "    if tag.startswith(\"J\"):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith(\"V\"):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith(\"N\"):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith(\"R\"):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def lemmatizza(tokens): #la funzione effettua la lemmatizzazione data la lista di tokens del corpus\n",
    "    tokens_no_punt = rimuovi_punteggiatura(tokens) #si rimuove la punteggiatura per evitare di considerare elementi che non sono lemmi\n",
    "    lemmatizzatore = WordNetLemmatizer()\n",
    "    lemmi = [lemmatizzatore.lemmatize(token,converti_PoS_in_wordnet(pos)) for token,pos in PoS_tagger(tokens_no_punt)] #mediante la sintassi della list comprehension è estratta una lista di lemmi a partire dalla lista (token,PoS) ottenuta con la funzione \"PoS_tagger\"\n",
    "    return lemmi #restituisce la lista dei lemmi\n",
    "\n",
    "print(\"5. Numero di lemmi distinti (i.e., la dimensione del vocabolario dei lemmi:\")\n",
    "\n",
    "print(f\"\\tIl vocabolario del {file1} ha un totale di {len(list(set(lemmatizza(rimuovi_punteggiatura(tokens1)))))} lemmi.\")\n",
    "print(f\"\\tIl vocabolario del {file2} ha un totale di {len(list(set(lemmatizza(rimuovi_punteggiatura(tokens2)))))} lemmi.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7 (v3.10.7:6cc6b13308, Sep  5 2022, 14:02:52) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
