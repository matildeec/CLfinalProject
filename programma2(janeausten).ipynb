{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analisi di ./janeausten.txt\n"
     ]
    }
   ],
   "source": [
    "def leggi_contenuto(file): #la funzione restituisce in output una variabile che ha memorizzato i contenuti del file con il metodo .read()\n",
    "    with open(file, \"r\") as infile:\n",
    "        contenuto = infile.read() #il metodo .read() consente di memorizzare il contenuto del file in una variabile\n",
    "    return contenuto\n",
    "\n",
    "file1 = \"./janeausten.txt\"\n",
    "corpus1 = leggi_contenuto(file1)\n",
    "\n",
    "print(f\"Analisi di {file1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def splitter_e_tokenizzatore(corpus): #la funzione effettua il sentence splitting e la tokenizzazione del corpus\n",
    "    frasi = nltk.tokenize.sent_tokenize(corpus) #splitta il testo in frasi\n",
    "    tokens = []\n",
    "    for frase in frasi:\n",
    "        tokens_frase = nltk.tokenize.word_tokenize(frase) #tokenizza ciascuna frase\n",
    "        tokens.extend(tokens_frase) # .extend() aggiunge in coda alla lista di tokens del corpus i tokens della frase\n",
    "    return frasi, tokens\n",
    "\n",
    "def PoS_tagger(tokens): #la funzione effettua il PoS tagging data la lista di tokens del corpus\n",
    "    tokens_PoS = nltk.tag.pos_tag(tokens)\n",
    "    return tokens_PoS #restituisce la lista di tuple (token,PoS)\n",
    "\n",
    "frasi1, tokens1 = splitter_e_tokenizzatore(corpus1)\n",
    "tokens_PoS1 = PoS_tagger(tokens1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 PoS più frequenti (con relativa frequenza)\n",
      "NN________________________________________________763\n",
      "IN________________________________________________662\n",
      ",_________________________________________________500\n",
      "DT________________________________________________495\n",
      "PRP_______________________________________________454\n",
      "RB________________________________________________437\n",
      "JJ________________________________________________404\n",
      "NNP_______________________________________________354\n",
      "VBD_______________________________________________337\n",
      "CC________________________________________________291\n",
      "\n",
      "10 bigrammi di PoS più frequenti (con relativa frequenza)\n",
      "('DT', 'NN')______________________________________252\n",
      "('NN', 'IN')______________________________________231\n",
      "('NN', ',')_______________________________________180\n",
      "('IN', 'DT')______________________________________169\n",
      "('JJ', 'NN')______________________________________163\n",
      "('PRP', 'VBD')____________________________________130\n",
      "('NNP', 'NNP')____________________________________124\n",
      "('DT', 'JJ')______________________________________120\n",
      "('TO', 'VB')______________________________________110\n",
      "('IN', 'PRP')_____________________________________109\n",
      "\n",
      "10 trigrammi di PoS più frequenti (con relativa frequenza)\n",
      "('DT', 'NN', 'IN')________________________________104\n",
      "('IN', 'DT', 'NN')________________________________102\n",
      "('DT', 'JJ', 'NN')________________________________79\n",
      "('NN', 'IN', 'DT')________________________________57\n",
      "('NN', ',', 'CC')_________________________________50\n",
      "('JJ', 'NN', 'IN')________________________________48\n",
      "('JJ', 'NN', ',')_________________________________47\n",
      "('DT', 'NN', ',')_________________________________42\n",
      "('IN', 'DT', 'JJ')________________________________38\n",
      "('NN', 'IN', 'PRP$')______________________________38\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "def stampa_lista_di_tuple(lista): #funzione ausiliaria che formatta la stampa in colonna di una lista di tuple costituite da 2 elementi\n",
    "    for i in range(len(lista)): #range(len(list)) crea una sequenza di numeri da 0 a len(list)-1\n",
    "        print(f\"{lista[i][0]}\".ljust(50,\"_\")+f\"{lista[i][1]}\") #il metodo .ljust() consente di giustificare a sinistra una stringa usando \"_\" come separatore\n",
    "\n",
    "def trova_ngrammiPoS_frequenti(tokens,n): #la funzione estrae i 10 n-grammi di PoS più frequenti\n",
    "    PoS_tags = [PoS for token,PoS in PoS_tagger(tokens)] #mediante la sintassi di list comprehension per ogni tupla (token,PoS) restituita dalla funzione \"PoS_tagger\" vengono estratti solo i PoS tags\n",
    "    if n==1: #se n=1 (unigrammi) calcola e ordina direttamente le frequenze dei tags in PoS_tags\n",
    "        ngrammi_PoS_frequenti = nltk.FreqDist(PoS_tags).most_common(10) #con \"FreqDist\" (funzione di libreria) si ottiene un dizionario di frequenze ordinato in maniera decrescente; con .most_common(10) si ottiene una lista dei primi 10 elementi\n",
    "    else: #se n!=1 (n-grammi) estrae prima gli n-grammi e poi calcola e ordina le frequenze\n",
    "        ngrammi_PoS = list(nltk.ngrams(PoS_tags, n)) #estrae n-grammi con il metodo nltk.ngrams()\n",
    "        ngrammi_PoS_frequenti = nltk.FreqDist(ngrammi_PoS).most_common(10)\n",
    "    return ngrammi_PoS_frequenti #restituisce la lista di 10 tuple (ngramma,frequenza) ordinate per frequenza decrescente\n",
    "\n",
    "print(\"10 PoS più frequenti (con relativa frequenza)\")\n",
    "stampa_lista_di_tuple(trova_ngrammiPoS_frequenti(tokens1,1))\n",
    "print(\"\")\n",
    "print(\"10 bigrammi di PoS più frequenti (con relativa frequenza)\")\n",
    "stampa_lista_di_tuple(trova_ngrammiPoS_frequenti(tokens1,2))\n",
    "print(\"\")\n",
    "print(\"10 trigrammi di PoS più frequenti (con relativa frequenza)\")\n",
    "stampa_lista_di_tuple(trova_ngrammiPoS_frequenti(tokens1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 Sostantivi più frequenti (con relativa frequenza)\n",
      "Mr._______________________________________________37\n",
      "Miss______________________________________________25\n",
      "father____________________________________________23\n",
      "Taylor____________________________________________23\n",
      "Emma______________________________________________21\n",
      "Elliot____________________________________________21\n",
      "Walter____________________________________________20\n",
      "years_____________________________________________18\n",
      "Sir_______________________________________________17\n",
      "Weston____________________________________________14\n",
      "man_______________________________________________14\n",
      "Knightley_________________________________________13\n",
      "Elizabeth_________________________________________13\n",
      "family____________________________________________11\n",
      "friend____________________________________________11\n",
      "Lady______________________________________________10\n",
      "Woodhouse_________________________________________9\n",
      "house_____________________________________________9\n",
      "time______________________________________________9\n",
      "thing_____________________________________________9\n",
      "\n",
      "20 Avverbi più frequenti (con relativa frequenza)\n",
      "not_______________________________________________55\n",
      "very______________________________________________40\n",
      "so________________________________________________26\n",
      "only______________________________________________17\n",
      "always____________________________________________17\n",
      "never_____________________________________________17\n",
      "again_____________________________________________14\n",
      "now_______________________________________________11\n",
      "up________________________________________________10\n",
      "ever______________________________________________9\n",
      "well______________________________________________8\n",
      "here______________________________________________7\n",
      "much______________________________________________6\n",
      "still_____________________________________________6\n",
      "more______________________________________________5\n",
      "away______________________________________________5\n",
      "soon______________________________________________5\n",
      "off_______________________________________________5\n",
      "even______________________________________________5\n",
      "as________________________________________________5\n",
      "\n",
      "20 Aggettivi più frequenti (con relativa frequenza)\n",
      "own_______________________________________________17\n",
      "good______________________________________________13\n",
      "great_____________________________________________12\n",
      "little____________________________________________10\n",
      "much______________________________________________9\n",
      "such______________________________________________9\n",
      "other_____________________________________________8\n",
      "poor______________________________________________8\n",
      "dear______________________________________________8\n",
      "more______________________________________________7\n",
      "many______________________________________________7\n",
      "sure______________________________________________7\n",
      "young_____________________________________________6\n",
      "last______________________________________________5\n",
      "most______________________________________________5\n",
      "happy_____________________________________________4\n",
      "best______________________________________________4\n",
      "excellent_________________________________________4\n",
      "real______________________________________________4\n",
      "long______________________________________________4\n"
     ]
    }
   ],
   "source": [
    "def trova_classePoS_frequenti(tokens_PoS,filtro_PoS): #la funzione estrae e stampa i 20 token più frequenti appartenenti a una certa classe di PoS dati la lista (token,PoS) e un filtro PoS\n",
    "    tokens_filtrati = [token for token,PoS in tokens_PoS if PoS.startswith(filtro_PoS)] #mediante la sintassi di list comprehension sono estratti i token appartenenti alla stessa classe di PoS filtrata con il metodo .startswith() perché a una PoS (es. Sostantivo) possono corrispondere più tags (es. \"NN\", \"NNP\", \"NNPS\", \"NNS\")\n",
    "    tokens_frequenti = nltk.FreqDist(tokens_filtrati).most_common(20) #con \"FreqDist\" (funzione di libreria) si ottiene un dizionario di frequenze ordinato in maniera decrescente; con .most_common(20) si ottiene una lista dei primi 20 elementi\n",
    "    return tokens_frequenti #restituisce la lista di 20 tuple (token,frequenza) ordinate per frequenza decrescente\n",
    "\n",
    "print(\"20 Sostantivi più frequenti (con relativa frequenza)\")\n",
    "stampa_lista_di_tuple(trova_classePoS_frequenti(tokens_PoS1,\"N\"))\n",
    "print(\"\")\n",
    "print(\"20 Avverbi più frequenti (con relativa frequenza)\")\n",
    "stampa_lista_di_tuple(trova_classePoS_frequenti(tokens_PoS1,\"R\"))\n",
    "print(\"\")\n",
    "print(\"20 Aggettivi più frequenti (con relativa frequenza)\")\n",
    "stampa_lista_di_tuple(trova_classePoS_frequenti(tokens_PoS1,\"J\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 20 bigrammi Aggettivo, Sostantivo più frequenti (con relativa frequenza)\n",
      "('poor', 'Miss')__________________________________4\n",
      "('young', 'man')__________________________________4\n",
      "('lucky', 'guess')________________________________3\n",
      "('excellent', 'woman')____________________________2\n",
      "('other', 'people')_______________________________2\n",
      "('great', 'deal')_________________________________2\n",
      "('odd', 'humours')________________________________2\n",
      "('intimate', 'friend')____________________________2\n",
      "('great', 'regard')_______________________________2\n",
      "('more', 'matches')_______________________________2\n",
      "('such', 'success')_______________________________2\n",
      "('good', 'looks')_________________________________2\n",
      "('thirteen', 'years')_____________________________2\n",
      "('comfortable', 'home')___________________________1\n",
      "('happy', 'disposition')__________________________1\n",
      "('best', 'blessings')_____________________________1\n",
      "('indulgent', 'father')___________________________1\n",
      "('early', 'period')_______________________________1\n",
      "('indistinct', 'remembrance')_____________________1\n",
      "('Sixteen', 'years')______________________________1\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def estrai_bigrammi_agg_sost(tokens): #la funzione estrae i bigrammi composti da aggettivo e sostantivo\n",
    "    bigrammi_token_PoS = list(nltk.bigrams(PoS_tagger(tokens))) #sono estratti i bigrammi di tuple (token,pos) ottenute dal PoS tagging effettuato sui tokens in input    \n",
    "    bigrammi_agg_sost = [(token1,token2) for (token1,PoS1),(token2,PoS2) in bigrammi_token_PoS if PoS1.startswith(\"J\") and PoS2.startswith(\"N\")] #mediante la sintassi di list comprehension sono estratti i bigrammi (token_aggettivo,token_sostantivo); è usato il metodo .startswith() perché a una PoS (es. Sostantivo) possono corrispondere più tags (es. \"NN\", \"NNP\", \"NNPS\", \"NNS\")\n",
    "    return bigrammi_agg_sost #restituisce la lista di tuple (token,token) di cui il primo è un aggettivo e il secondo è un sostantivo\n",
    "\n",
    "def trova_bigrammi_frequenti(bigrammi): #la funzione restituisce i bigrammi più frequenti data una lista di bigrammi\n",
    "    bigrammi_frequenti = nltk.FreqDist(bigrammi).most_common(20) #con \"FreqDist\" (funzione di libreria) si ottiene un dizionario di frequenze ordinato in maniera decrescente; con .most_common(20) si ottiene una lista dei primi 20 elementi\n",
    "    return bigrammi_frequenti #restituisce la lista delle prime 20 tuple (bigramma,frequenza) ordinate per frequenza decrescente\n",
    "\n",
    "print(\"I 20 bigrammi Aggettivo, Sostantivo più frequenti (con relativa frequenza)\")\n",
    "stampa_lista_di_tuple(trova_bigrammi_frequenti(estrai_bigrammi_agg_sost(tokens1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 20 bigrammi Aggettivo, Sostantivo con probabilità condizionata massima (e relativo valore)\n",
      "('odd', 'humours')________________________________1.0\n",
      "('indulgent', 'father')___________________________1.0\n",
      "('indistinct', 'remembrance')_____________________1.0\n",
      "('Sixteen', 'years')______________________________1.0\n",
      "('nominal', 'office')_____________________________1.0\n",
      "('mournful', 'thought')___________________________1.0\n",
      "('unexceptionable', 'character')__________________1.0\n",
      "('easy', 'fortune')_______________________________1.0\n",
      "('suitable', 'age')_______________________________1.0\n",
      "('generous', 'friendship')________________________1.0\n",
      "('past', 'kindness')______________________________1.0\n",
      "('various', 'illnesses')__________________________1.0\n",
      "('tenderer', 'recollection')______________________1.0\n",
      "('intellectual', 'solitude')______________________1.0\n",
      "('actual', 'disparity')___________________________1.0\n",
      "('older', 'man')__________________________________1.0\n",
      "('daily', 'reach')________________________________1.0\n",
      "('populous', 'village')___________________________1.0\n",
      "('separate', 'lawn')______________________________1.0\n",
      "('melancholy', 'change')__________________________1.0\n"
     ]
    }
   ],
   "source": [
    "def trova_bigrammi_probCondizionata_massima(bigrammi,tokens): #la funzione restituisce i bigrammi con Probabilità Condizionata massima data la lista di bigrammi e la lista di tokens\n",
    "    bigrammi_frequenza = nltk.FreqDist(bigrammi).most_common() #il metodo .most_common() privo di argomento restituisce la lista completa di tuple (bigramma,frequenza)\n",
    "    bigrammi_probabilita = {} #crea un dizionario per memorizzare la probabilità di ciascun bigramma\n",
    "    for bigramma,frequenza in bigrammi_frequenza: #si scorre la lista \"bigrammi_frequenza\"\n",
    "        frequenza_parola1 = tokens.count(bigramma[0]) #si ottiene la frequenza della prima parola del bigramma (che è una tupla (token,token))\n",
    "        probabilita_bigramma = frequenza/frequenza_parola1 #calcola la probabilità condizionata del bigramma\n",
    "        bigrammi_probabilita.update({bigramma:probabilita_bigramma}) #aggiorna il dizionario con il nuovo valore {bigramma:probabilità}\n",
    "    bigrammi_probabilita_decrescente = sorted(bigrammi_probabilita.items(), key=lambda x:x[1], reverse=True) #a partire dal dizionario di ottiene una lista ordinata per valori decrescenti con la funzione di libreria \"sorted\"\n",
    "    return bigrammi_probabilita_decrescente[:20] #si stampano i primi 20 elementi della lista con lo slicing\n",
    "\n",
    "print(\"I 20 bigrammi Aggettivo, Sostantivo con probabilità condizionata massima (e relativo valore)\")\n",
    "stampa_lista_di_tuple(trova_bigrammi_probCondizionata_massima(estrai_bigrammi_agg_sost(tokens1),tokens1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 20 bigrammi Aggettivo, Sostantivo con Pointwise Mutual Information massima (e relativo valore)\n",
      "('generous', 'friendship')________________________12.737\n",
      "('various', 'illnesses')__________________________12.737\n",
      "('tenderer', 'recollection')______________________12.737\n",
      "('intellectual', 'solitude')______________________12.737\n",
      "('actual', 'disparity')___________________________12.737\n",
      "('daily', 'reach')________________________________12.737\n",
      "('separate', 'lawn')______________________________12.737\n",
      "('mutual', 'connexions')__________________________12.737\n",
      "('beautiful', 'moonlight')________________________12.737\n",
      "('solemn', 'nonsense')____________________________12.737\n",
      "('worthy', 'employment')__________________________12.737\n",
      "('limited', 'remnant')____________________________12.737\n",
      "('earliest', 'patents')___________________________12.737\n",
      "('endless', 'creations')__________________________12.737\n",
      "('successive', 'parliaments')_____________________12.737\n",
      "('Principal', 'seat')_____________________________12.737\n",
      "('Few', 'women')__________________________________12.737\n",
      "('constant', 'object')____________________________12.737\n",
      "('youthful', 'infatuation')_______________________12.737\n",
      "('Three', 'girls')________________________________12.737\n"
     ]
    }
   ],
   "source": [
    "def trova_bigrammi_PMI_massima(bigrammi,tokens): #la funzione restituisce i bigrammi con Pointwise Mutual Information massima data la lista di bigrammi e la lista di tokens\n",
    "    bigrammi_frequenza = nltk.FreqDist(bigrammi).most_common() #il metodo .most_common() privo di argomento restituisce la lista completa di tuple (bigramma,frequenza)\n",
    "    bigrammi_PMI = {} #crea un dizionario per memorizzare la PMI di ciascun bigramma\n",
    "    for bigramma,frequenza in bigrammi_frequenza: #si scorre la lista \"bigrammi_frequenza\"\n",
    "        frequenza_parola1 = tokens.count(bigramma[0]) #si ottiene la frequenza della prima parola del bigramma (che è una tupla (token,token))\n",
    "        frequenza_parola2 = tokens.count(bigramma[1]) #si ottiene la frequenza della seconda parola del bigramma\n",
    "        PMI_bigramma = round(math.log2((frequenza*len(tokens1))/(frequenza_parola1*frequenza_parola2)),3) #calcola la PMI del bigramma (NB: tokens1 è una variabile globale)\n",
    "        bigrammi_PMI.update({bigramma:PMI_bigramma}) #aggiorna il dizionario con il nuovo valore {bigramma:PMI_bigramma}\n",
    "    bigrammi_PMI_decrescente = sorted(bigrammi_PMI.items(), key=lambda x:x[1], reverse=True) #a partire dal dizionario di ottiene una lista ordinata per valori decrescenti con la funzione di libreria \"sorted\"\n",
    "    return bigrammi_PMI_decrescente[:20] #si stampano i primi 20 elementi della lista con lo slicing\n",
    "\n",
    "print(\"I 20 bigrammi Aggettivo, Sostantivo con Pointwise Mutual Information massima (e relativo valore)\")\n",
    "stampa_lista_di_tuple(trova_bigrammi_PMI_massima(estrai_bigrammi_agg_sost(tokens1),tokens1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ottieni_frasi_tokenizzate(frasi): #funzione ausiliaria che restituisce un dizionario di frasi tokenizzate\n",
    "    frasi_tokenizzate = {}\n",
    "    for frase in frasi: #si scorre la lista di frasi in input\n",
    "        tokens_frase = nltk.tokenize.word_tokenize(frase) #tokenizza ciascuna frase\n",
    "        frasi_tokenizzate.update({frase:tokens_frase}) #il dizionario è aggiornato con la nuova coppia {chiave:valore}\n",
    "    return frasi_tokenizzate #restituisce un dizionario in cui le chiavi sono le frasi originarie e i valori sono le frasi tokenizzate individualmente\n",
    "\n",
    "def trova_hapax(tokens): #funzione ausiliaria che restituisce in output la lista di hapax\n",
    "    hapax = [token for token in list(set(tokens)) if tokens.count(token) == 1] #mediante la sintassi di list comprehension si estraggono i token che ricorrono solo 1 volta; si usa list(set(tokens)) per ottenere il vocabolario di parole tipo\n",
    "    return hapax\n",
    "\n",
    "def seleziona_frasi_10_20(corpus): #la funzione seleziona le frasi con lunghezza compresa tra 10 e 20 token, di cui la metà NON sono hapax\n",
    "    frasi,tokens = splitter_e_tokenizzatore(corpus) #estrae frasi e tokens dal corpus\n",
    "    frasi_tokenizzate = ottieni_frasi_tokenizzate(frasi) #ottiene il dizionario di frasi tokenizzate\n",
    "    hapax = trova_hapax(tokens) #ottiene la lista di hapax del corpus\n",
    "    frasi_selezionate = []\n",
    "    for frase,tokens_frase in frasi_tokenizzate.items(): #si scorre il dizionario\n",
    "        if len(tokens_frase)>=10 and len(tokens_frase)<=20: #vengono prese in considerazione solo le frasi la cui lunghezza è compresa tra 10 e 20 token\n",
    "            i = 0 #si inizializza a 0 una variabile i (contatore) che tiene il conto dei token che NON sono hapax\n",
    "            for token in tokens_frase: #si scorre la lista di tokens (di ciascuna frase)\n",
    "                if token not in hapax: #se il token NON è presente nella lista di hapax il contatore si aggiorna\n",
    "                    i += 1\n",
    "            if i >= math.floor(len(tokens_frase)/2): #se il valore del contatore è almeno pari alla metà della lunghezza della frase allora la frase viene aggiunta alla lista \"frasi_selezionate\"\n",
    "                frasi_selezionate.append(frase)\n",
    "    return frasi_selezionate #restituisce la lista contenente le frasi selezionate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La frase con la media della distribuzione di frequenza dei token più alta, ovvero 133.909, è:\n",
      "\t\"No, papa, nobody thought of your walking.\n",
      "La frase con la media della distribuzione di frequenza dei token più bassa, ovvero 26.3, è:\n",
      "\tA worthy employment for a young lady's mind!\n"
     ]
    }
   ],
   "source": [
    "def media_distrFreq_max_min(frasi,tokens): #la funzione calcola la media della distribuzione di frequenza dei token per ogni frase della lista in input e restituisce le frasi con il valore massimo e minimo\n",
    "    frasi_tokenizzate = ottieni_frasi_tokenizzate(frasi) #si ottiene il dizionario di frasi tokenizzate\n",
    "    max = 0 #si inizializza a 0 il valore massimo\n",
    "    min = float(\"inf\") #si inizializza a infinito il valore minimo\n",
    "    for frase,tokens_frase in frasi_tokenizzate.items(): #si scorre il dizionario\n",
    "        somma = 0 #si inizializza a 0 la variabile che contiene la somma\n",
    "        for token in tokens_frase: #si scorre la lista di tokens (per ciascuna frase)\n",
    "            frequenza_token = tokens.count(token) #si ottiene la frequenza del token nell'intero corpus\n",
    "            somma += frequenza_token #somma le frequenze di ciascun token della frase\n",
    "        media_distrFreq_frase = round(somma/len(tokens_frase),3) #calcola la media della distribuzione di frequenza dei token per ogni frase, ovvero: si divide il totale della frequenza di tutti i token per il numero di token (il valore è arrotondato a 3 cifre decimali)\n",
    "        \n",
    "        if media_distrFreq_frase > max: #se il valore è maggiore del valore massimo, quest'ultimo viene aggiornato\n",
    "            max = media_distrFreq_frase\n",
    "            frase_max = frase #memorizza la frase con il valore massimo\n",
    "\n",
    "        if media_distrFreq_frase < min: #se il valore è minore del valore minimo, quest'ultimo viene aggiornato\n",
    "            min = media_distrFreq_frase\n",
    "            frase_min = frase #memorizza la frase con il valore minimo\n",
    "\n",
    "    return frase_max, max, frase_min, min\n",
    "\n",
    "frase_max1,max1,frase_min1,min1 = media_distrFreq_max_min(seleziona_frasi_10_20(corpus1),tokens1)\n",
    "print(f\"La frase con la media della distribuzione di frequenza dei token più alta, ovvero {max1}, è:\\n\\t{frase_max1}\")\n",
    "print(f\"La frase con la media della distribuzione di frequenza dei token più bassa, ovvero {min1}, è:\\n\\t{frase_min1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La frase con la probabilità più alta secondo il modello di Markov di ordine 2, ovvero 7.323861139592795e-05, è:\n",
      "\tI only doubt whether he will ever take us anywhere else.\n"
     ]
    }
   ],
   "source": [
    "def markov2_max(frasi,tokens): #la funzione calcola la probabilità delle frasi secondo un modello di Markov di ordine 2 e restituisce quella con probabilità massima \n",
    "    frasi_tokenizzate = ottieni_frasi_tokenizzate(frasi) #si ottiene il dizionario di frasi tokenizzate\n",
    "    bigrammi_corpus = list(nltk.bigrams(tokens)) #estrae i bigrammi di token dall'intero corpus\n",
    "    trigrammi_corpus = list(nltk.trigrams(tokens)) #estrae i trigrammi di token dall'intero corpus\n",
    "    max = 0 #si inizializza a 0 il valore massimo\n",
    "\n",
    "    for frase,tokens_frase in frasi_tokenizzate.items(): #si scorre il dizionario\n",
    "        bigrammi_frase = list(nltk.bigrams(tokens_frase)) #estrae i bigrammi di token dalla frase\n",
    "        trigrammi_frase = list(nltk.trigrams(tokens_frase)) #estrae i trigrammi di token dalla frase\n",
    "\n",
    "        prob_parola1 = tokens.count(tokens_frase[0])/len(tokens) #si calcola la probabilità della prima parola della catena di Markov\n",
    "        prob_parola2 = (bigrammi_corpus.count(bigrammi_frase[0]))/tokens.count(tokens_frase[0]) #si calcola la probabilità della seconda parola della catena di Markov\n",
    "        prodotto = prob_parola1 * prob_parola2 #si inizializza una variabile che dovrà contenere il prodotto complessivo della catena di Markov\n",
    "        \n",
    "        for i in range(len(trigrammi_frase)): #si scorrono i trigrammi della frase\n",
    "            frequenza_trigramma = trigrammi_corpus.count(trigrammi_frase[i]) #estrae la frequenza del trigramma\n",
    "            frequenza_bigramma = bigrammi_corpus.count(bigrammi_frase[i]) #estrae la frequenza del bigramma\n",
    "            prob_condizionata_parola = frequenza_trigramma/frequenza_bigramma #calcola la probabilità condizionata della seguente parola (dalla terza in poi) nella catena di Markov\n",
    "            prodotto *= prob_condizionata_parola #il valore è moltiplicato al prodotto complessivo\n",
    "        \n",
    "        if prodotto > max: #se il prodotto complessivo è maggiore del valore massimo, quest'ultimo si aggiorna\n",
    "            max = prodotto\n",
    "            frase_max = frase #si memorizza la frase corrispondete al prodotto massimo\n",
    "            \n",
    "    return frase_max, max\n",
    "\n",
    "frase_max_markov, max_markov = markov2_max(seleziona_frasi_10_20(corpus1),tokens1)\n",
    "print(f\"La frase con la probabilità più alta secondo il modello di Markov di ordine 2, ovvero {max_markov}, è:\\n\\t{frase_max_markov}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERSON:\n",
      "Miss Taylor_______________________________________13\n",
      "Mr. Knightley_____________________________________13\n",
      "Elizabeth_________________________________________13\n",
      "Emma______________________________________________12\n",
      "Mr. Weston________________________________________12\n",
      "Sir Walter________________________________________12\n",
      "Mr. Woodhouse_____________________________________6\n",
      "Lady Elliot_______________________________________6\n",
      "Anne______________________________________________5\n",
      "Mary______________________________________________5\n",
      "Lady Russell______________________________________5\n",
      "James_____________________________________________4\n",
      "Mr. Elton_________________________________________4\n",
      "Sir Walter Elliot_________________________________4\n",
      "Hartfield_________________________________________3\n",
      "\n",
      "GPE:\n",
      "London____________________________________________4\n",
      "Emma______________________________________________3\n",
      "Weston____________________________________________2\n",
      "Isabella__________________________________________2\n",
      "Highbury__________________________________________2\n",
      "Somerset__________________________________________2\n",
      "Elliot____________________________________________2\n",
      "Kellynch__________________________________________2\n",
      "Randalls__________________________________________1\n",
      "Hannah____________________________________________1\n",
      "Brunswick Square__________________________________1\n",
      "Emma Woodhouse____________________________________1\n",
      "Broadway Lane_____________________________________1\n",
      "Somersetshire_____________________________________1\n",
      "Baronetage________________________________________1\n",
      "\n",
      "ORGANIZATION:\n",
      "Miss Taylor_______________________________________9\n",
      "Hartfield_________________________________________2\n",
      "Kellynch Hall_____________________________________2\n",
      "Woodhouses________________________________________1\n",
      "Success___________________________________________1\n",
      "Gloucester________________________________________1\n",
      "Marys_____________________________________________1\n",
      "Sir_______________________________________________1\n",
      "Miss Elliot_______________________________________1\n",
      "Tattersall________________________________________1\n",
      "House_____________________________________________1\n",
      "Commons___________________________________________1\n",
      "Baronetage________________________________________1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def ottieni_NE(tokens_PoS): #la funzione ottiene un dizionario di NE a partire dalla lista di tuple (token,PoS)\n",
    "    albero_NE = nltk.ne_chunk(tokens_PoS) #si genera l'albero delle Named Entities a partire dalla lista di tuple (token,PoS)\n",
    "    NE = {} #viene creato un dizionario vuoto\n",
    "    for nodo in albero_NE: #si scorre l'albero attraverso i nodi\n",
    "        if hasattr(nodo, 'label'): #se il nodo ha attributo 'label' si accede all'attributo label del nodo, che contiene la classe dell'entità\n",
    "            classe = nodo.label() #memorizza il nome della classe\n",
    "            entita = \" \".join([token for token,PoS in nodo.leaves()]) #ricostruisce la stringa completa dell'entità\n",
    "            if classe in NE.keys(): #se \"classe\" è tra le chiavi del dizionario NE allora la lista di entità corrispondenti viene aggiornata con il nuovo elemento\n",
    "                NE[classe].append(entita)\n",
    "            else: #altrimenti \"classe\" diventa una nuova chiave del dizionario\n",
    "                NE[classe] = [entita]\n",
    "    return NE #restituisce un dizionario in cui le chiavi sono le classi di NE e i valori sono liste contenenti i tokens appartenenti alla classe\n",
    "\n",
    "def trova_NE_frequenti(NE,classe): #la funzione restituisce gli elementi più frequenti (con relativa frequenza) per una classe di NE\n",
    "    FreqDist_classe_NE = FreqDist(NE[classe]).most_common(15)\n",
    "    return FreqDist_classe_NE #restituisce la lista delle prime 15 tuple (entità,frequenza) ordinate per frequenza decrescente\n",
    "\n",
    "for classe in ottieni_NE(tokens_PoS1).keys(): #si scorrono le chiavi del dizionario NE e per ogni classe di NE si stampano le entità più frequenti trovate\n",
    "    print(f\"{classe}:\")\n",
    "    stampa_lista_di_tuple(trova_NE_frequenti(ottieni_NE(tokens_PoS1),classe))\n",
    "    print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
